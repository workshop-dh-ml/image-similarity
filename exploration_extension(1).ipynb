{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your dataset: The folder 'sphaera_illustrations_dataset' contains 844 illustrations, selected from all illustrations that have been extracted from the books in the sphaera corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cryptographic hashing for duplicate detection\n",
    "\n",
    "Datasets are never clean. In image datasets it might be a good idea to check for (exact) duplicates before starting to analyze the data.\n",
    "\n",
    "Cryptographic hashing techniques produce a fixed size fingerprint of an inputfile in such a way, that small changes in the file result in large changes in the hash. To find identical files, we can thus compare the hashes instead of the files them selfs, which due to the small size of the hashes is much faster.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "We have hidden on duplicate file in the datatset. To find it:\n",
    "\n",
    "* hash all files in the dataset with the md5 hash function (use the hashlib library)\n",
    "* compare the hashes to find the duplicate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "\n",
    "dataset_dir = '/home/space/datasets/workshop-dh-ml/image-similarity/sphaera_illustrations_dataset' \n",
    "hash_to_filename = {}\n",
    "\n",
    "for filename in os.listdir(dataset_dir):\n",
    "    \n",
    "    if filename == '.DS_Store':\n",
    "        pass # This is for Mac users. Do nothing if your folder contains the notorious .DS_Store file\n",
    "        \n",
    "    else:\n",
    "        filepath = os.path.join(dataset_dir, filename)\n",
    "        filehash = hashlib.md5(open(filepath,  'rb').read()).hexdigest()\n",
    "        \n",
    "        if filehash in hash_to_filename.keys():\n",
    "            print('We found a duplicate pair: {} and {}'.format(filename, hash_to_filename[filehash]))\n",
    "        else: \n",
    "            hash_to_filename[filehash] = filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hash an image\n",
    "\n",
    "In image hashing or fingerprinting, opposed to cryptographic hashing we want to produce fingerprints which are not very sensitive to little changes such that similar images produce similar hashes.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "To hash an image: \n",
    "\n",
    "* Read in \"2106_sacrobosco_sphaera_1561_p25_348,1242,509,433.jpg\" with PIL. Have a look at the image.\n",
    "* Convert to grayscale\n",
    "* Scale to 8x 8 pixel\n",
    "* Compute the mean pixe intensity (hint: use numpy)\n",
    "* Convert every pixel with less than mean intensity to black and every pixel grater equal than than mean intensity to white\n",
    "* Visualize result\n",
    "* Flatten the resulting image\n",
    "\n",
    "If you succeed you have created your first image hash. This particular has function is called average hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def get_avarage_hash(filename):\n",
    "    \n",
    "    filepath = os.path.join(dataset_dir, filename)\n",
    "    img = Image.open(filepath)\n",
    "    img2 = ImageOps.grayscale(img)\n",
    "    img3 = img2.resize((8,8))\n",
    "\n",
    "    avarage_hash = np.where(img3 > np.array(img3).mean(), 1, 0)\n",
    "\n",
    "    return img, avarage_hash\n",
    "\n",
    "\n",
    "img_1, avarage_hash_1 = get_avarage_hash(\"1701_sacrobosco_sphaera_1550_p40_395,1295,501,430.jpg\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(img_1, cmap='gray')\n",
    "ax2.imshow(avarage_hash_1, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print('Here is our first image hash: {}'.format(avarage_hash_1.flatten()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compare  images via their hashes\n",
    "\n",
    "#### Task:\n",
    "\n",
    "* Calculate and visualize the average hash for \"2106_sacrobosco_sphaera_1561_p25_348,1242,509,433.jpg\" \n",
    "* Compare it to the hash calculated before, visualize the pixels where the two hashes are different.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_2, avarage_hash_2 = get_avarage_hash(\"2106_sacrobosco_sphaera_1561_p25_348,1242,509,433.jpg\")\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "ax1.imshow(img_1, cmap='gray')\n",
    "ax2.imshow(avarage_hash_1, cmap='gray')\n",
    "ax3.imshow(img_2, cmap='gray')\n",
    "ax4.imshow(avarage_hash_2, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "diff = avarage_hash_1 == avarage_hash_2\n",
    "plt.imshow(diff, cmap=\"Blues_r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the average hash we have indeed engineered a function that has  mapped our two _visually_ similar imaged onto two representation that are indeed _bitwise_ similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use image hashing to find  images in the corpus that are similar to a given image.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "Find all images in the corpus whose average hashes differ from that of our original image (\"2106_sacrobosco_sphaera_1561_p25_348,1242,509,433.jpg\") by no more than 10 bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for filename in os.listdir(dataset_dir):\n",
    "    \n",
    "    if filename == '.DStore':\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        img , avarage_hash = get_avarage_hash(filename)\n",
    "        hamming_dist = np.count_nonzero(avarage_hash_1.flatten() != avarage_hash.flatten())\n",
    "        \n",
    "        if hamming_dist  <= 10:\n",
    "            \n",
    "            print(re.sub(r'_\\d*,.*', '', filename))\n",
    "            filepath = os.path.join(dataset_dir, filename)\n",
    "            img = Image.open(filepath)\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the basic idea of finding similar images by means of image hashing. Of course there is a lot we can do to improve. In our case for instance we are only interested in what was actually printed, and not in the background information such as paper color or texture. We should thus for instance convert to black and white before comparing.\n",
    "\n",
    "Also there are image hashing functions such as the perceptual hash, which, depending on the nature of your data, may be better suited. Of course there is a library for everything in Python, so you do not have to implement them yourself. Have a look at: https://pypi.org/project/ImageHash/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Limits of image hashing \n",
    "\n",
    "Image hashing is a quick solution for retrieving similar images, but it is also rather crude with some severe limitation. Thus it is not invariant against translations or rotations of the images, which however, is often desirable when searching for similar imaged. It its also for instance not sensitive to the similarity between and image and parts thereof, for instance different crops.\n",
    "\n",
    "#### Task:\n",
    "* take our original image (\"2106_sacrobosco_\n",
    "sphaera_1561_p25_348,1242,509,433.jpg\" ) and cut of a quarter of its width on the right\n",
    "* calculate the average hash and compare to the average hash of the original\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_width = int(np.array(img_1).shape[1] - np.array(img_1).shape[1]/4)\n",
    "img_cut = np.array(img_1)[:,:new_width]\n",
    "\n",
    "img2 = ImageOps.grayscale(Image.fromarray(img_cut))\n",
    "img3 = img2.resize((8,8))\n",
    "avarage_hash_4 = np.where(img3 > np.array(img3).mean(), 1, 0)\n",
    "\n",
    "fig, ((ax1, ax2),(ax3, ax4)) = plt.subplots(2, 2)\n",
    "ax1.imshow(img_cut, cmap='gray')\n",
    "ax2.imshow(avarage_hash_4, cmap='gray')\n",
    "ax3.imshow(img_1, cmap='gray')\n",
    "ax4.imshow(avarage_hash_2, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "diff = avarage_hash_1 == avarage_hash_4\n",
    "plt.imshow(diff, cmap=\"Blues_r\")\n",
    "plt.show()\n",
    "\n",
    "hamming_dist = np.count_nonzero(avarage_hash_1.flatten() != avarage_hash_4.flatten())\n",
    "print('The original and the cropped image differ by {} bits.'.format(hamming_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional task: \n",
    "* Go back to 4. and find all images whose avarage hashes differ from that of our original image by 16 or less bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keypoint/feature detection and matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fingerprint matching as a blueprint for the approach\n",
    "|  |  |  |\n",
    "|---|---|---|\n",
    "||<img src=fingerprint.jpg>||\n",
    "|&nbsp;|<small>Srinivasan, Harish & J. Beal, Matthew & Srihari, Sargur. (2005). Machine learning approaches for person identification and verification. Proceedings of SPIE - The International Society for Optical Engineering. 5778.</small>|&nbsp;|\n",
    "\n",
    "* find robust features in finger ridge patterns (minutae, e.g. ridge endings or bifurcations) \n",
    "* match features\n",
    "* count the number of matches\n",
    "\n",
    "> ... the probability that a fingerprint with 36 minutiae points will share 12 minutiae points with another arbitrarily chosen fingerprint with 36 minutiae points is 6.10 × 10<sup>−8</sup>.\n",
    "\n",
    " \n",
    "<small>Sharath Pankanti, Salil Prabhakar, and Anil K. Jain. 2002. On the Individuality of Fingerprints. IEEE Trans. Pattern Anal. Mach. Intell. 24, 8 (August 2002), 1010-1025. </small>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Keypoint extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task:\n",
    "\n",
    "* Read in \"1852_sacrobosco_commentaria_1578_p215_498,157,604,604.jpg\", find robust keypoints and visualize some of them\n",
    "\n",
    "A number of algorithms for local feature detection and description are available, e.g. SURF (speeded up robust features) and SIFT (scale-invariant feature transform). ORB (Oriented FAST and rotated BRIEF) is a free alternative, part of the openCV computer vision library. A good introduction to keypoint extraction and matching can be found here: https://docs.opencv.org/master/db/d27/tutorial_py_table_of_contents_feature2d.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "filepath = os.path.join(dataset_dir, '1852_sacrobosco_commentaria_1578_p215_498,157,604,604.jpg')\n",
    "img_1 = cv2.imread(filepath)\n",
    "\n",
    "kp1, des1 = orb.detectAndCompute(img_1,None)\n",
    "pts = np.array([kp1[i].pt for i in range(0, len(kp1))]).reshape(-1, 2)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(pts[:40,0],pts[:40,1], marker='o', facecolors='none', edgecolors='r')\n",
    "plt.imshow(img_1) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task:\n",
    "\n",
    "* find keypoints and descriptors for a second image \"2203_giuntini_commentaria_1577_p219_523,108,502,507.jpg\"\n",
    "* match the keypoints between the two images (use BFMatcher https://docs.opencv.org/4.5.2/d3/da1/classcv_1_1BFMatcher.html)\n",
    "* visualize the 20 best matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(dataset_dir, '2203_giuntini_commentaria_1577_p219_523,108,502,507.jpg')\n",
    "img_2 = cv2.imread(filepath)\n",
    "\n",
    "kp2, des2 = orb.detectAndCompute(img_2,None)\n",
    "\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING , crossCheck=True)\n",
    "matches = bf.match(des1,des2)\n",
    "matches = np.array(sorted(matches, key = lambda x:x.distance))\n",
    "img_matches = cv2.drawMatches(img_1,kp1,img_2,kp2,matches[0:100], None, matchColor = (255,0,0),flags=2)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img_matches)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of ways to go from here. A simple one is to count the number of good matches between to images of a certain quality.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "* match the features of \"1959_vinet_sphaera_1594_p112_119,82,560,695.jpg\" as a reference file to the features of every other image in your dataset\n",
    "* count the number of good matches (match.distance <50)\n",
    "* visualize the images that have at least 50 good keypoint matches with the reference file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(dataset_dir, '1959_vinet_sphaera_1594_p112_119,82,560,695.jpg')\n",
    "img_1 = cv2.imread(filepath)\n",
    "\n",
    "kp1, des1 = orb.detectAndCompute(img_1,None)\n",
    "\n",
    "\n",
    "for j, filename in enumerate(os.listdir(dataset_dir)):\n",
    "    \n",
    "    if filename == '.DStore':\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        #print(j, end='\\r')\n",
    "        filepath = os.path.join(dataset_dir, filename)\n",
    "        img_2 = cv2.imread(filepath)\n",
    "\n",
    "        kp2, des2 = orb.detectAndCompute(img_2,None)\n",
    "        matches, good = bf.match(des1,des2), []\n",
    "    \n",
    "        count = 0\n",
    "    \n",
    "        for m in matches:\n",
    "            if m.distance < 50:\n",
    "                good.append(m)\n",
    "                \n",
    "        \n",
    "                \n",
    "        if len(good) > 50:\n",
    "            print(filepath)\n",
    "            #img = Image.open(filepath)\n",
    "            count += 1\n",
    "            plt.imshow(img_2)\n",
    "            plt.show()\n",
    "            \n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A slightly better version with FLANN matcher\n",
    "\n",
    "filepath = os.path.join(dataset_dir, '1959_vinet_sphaera_1594_p112_119,82,560,695.jpg')\n",
    "img_1 = cv2.imread(filepath)\n",
    "\n",
    "kp1, des1 = orb.detectAndCompute(img_1,None)\n",
    "\n",
    "\n",
    "for j, filename in enumerate(os.listdir(dataset_dir)):\n",
    "    \n",
    "    if filename == '.DStore':\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        filepath = os.path.join(dataset_dir, filename)\n",
    "        img_2 = cv2.imread(filepath)\n",
    "\n",
    "        kp2, des2 = orb.detectAndCompute(img_2,None)\n",
    "        index_params= dict(algorithm = 6,\n",
    "                           table_number = 6, # 12\n",
    "                           key_size = 12,     # 20\n",
    "                           multi_probe_level = 1) #2\n",
    "\n",
    "        search_params = dict(checks = 50)\n",
    "        flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "        matches = flann.knnMatch(des1,des2,k=2)\n",
    "\n",
    "\n",
    "        # Need to draw only good matches, so create a mask\n",
    "        try:\n",
    "            matchesMask = [[0,0] for i in range(len(matches))]\n",
    "            good = []\n",
    "            for i,(m,n) in enumerate(matches):\n",
    "                if m.distance < 0.7*n.distance:\n",
    "                    matchesMask[i]=[1,0]\n",
    "                    good.append([m])\n",
    "                    \n",
    "            draw_params = dict(matchColor = (0,255,0),\n",
    "                               singlePointColor = (255,0,0),\n",
    "                               matchesMask = matchesMask,\n",
    "                               flags = cv2.DrawMatchesFlags_DEFAULT)\n",
    "\n",
    "\n",
    "            if len(good) > 16:\n",
    "                img3 = cv2.drawMatchesKnn(img_1,kp1,img_2,kp2,matches,None,**draw_params)\n",
    "                plt.imshow(img3,)\n",
    "                plt.show()\n",
    "                print('similar to {}'.format(filename))\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN embedding distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Embedding\n",
    "\n",
    "During training CNNs learn to build up rich representations of the input images. This can be exploited for detecting similarity by cutting a pretrained CNN at a certain layer and calculating the distance between the embeddings of two images at that layer. The smaller the distance the higher (hopefully) the similarity. The deeper (further away from input) one moves into the network  the more complex and abstract and the less local  the features that make up these representations (embeddings) become. Thus we expect that the similarity we recover with this method moves from from concrete to more abstract if we use embeddings produced deeper into the network.\n",
    "\n",
    "#### Task:\n",
    "\n",
    "* cut a VGG16 pretrained on image net after the fourth convolutional block, i.e at layer 13 \n",
    "* calculate the embeddings for all images in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras import backend as K\n",
    "from keras.layers import merge, Input\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "\n",
    "cut_layer = 14\n",
    "\n",
    "\n",
    "image_input = Input(shape=(224, 224, 3))\n",
    "model = VGG16(include_top=True, weights=\"imagenet\",\n",
    "              input_tensor=image_input)  \n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Will cut at layer {}'.format(model.layers[cut_layer].output))\n",
    "\n",
    "inp = model.input  # input placeholder\n",
    "outputs = model.layers[cut_layer].output\n",
    "print(inp)\n",
    "print(outputs)\n",
    "functor = K.function([inp], [outputs])  # evaluation function\n",
    "\n",
    "layer_outs_arr = []\n",
    "\n",
    "for j, filename in enumerate(os.listdir(dataset_dir)):\n",
    "    \n",
    "    if filename == '.DStore':\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        filepath = os.path.join(dataset_dir, filename)\n",
    "        if (j % 100) == 0:\n",
    "            print(j, filepath)\n",
    "\n",
    "        img = image.load_img(filepath, target_size=(224, 224))\n",
    "        img_data = image.img_to_array(img)\n",
    "        img_data = np.expand_dims(img_data, axis=0)  # simply creates batch of size one as required by preprocess_input\n",
    "        img_data = tf.keras.applications.vgg16.preprocess_input(img_data)\n",
    "        layer_out = functor([img_data, 1])\n",
    "        layer_out = np.array(layer_out)\n",
    "        \n",
    "        layer_outs_arr.append(layer_out.flatten())\n",
    "\n",
    "layer_outs_arr = np.array(layer_outs_arr)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering\n",
    "\n",
    "#### Task:\n",
    "\n",
    "* cluster the embeddings using k-means with  15 clusters \n",
    "* visualize some of the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "km = KMeans(init='k-means++', n_clusters=15, \n",
    "                        verbose=1, n_init=3, max_iter=300)\n",
    "clustering = km.fit(layer_outs_arr)\n",
    "predictions = clustering.predict(layer_outs_arr)\n",
    "predictions, predictions.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for image in np.array(os.listdir(dataset_dir))[np.where(predictions==1)]:\n",
    "    filepath = os.path.join(dataset_dir, image)\n",
    "    img = cv2.imread(filepath)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    for i in range(15):\n",
    "        for image in np.array(os.listdir(dataset_dir))[np.where(predictions==i)]:\n",
    "            filepath = os.path.join(dataset_dir, image)\n",
    "            img = cv2.imread(filepath)\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "km = KMeans(init='k-means++', n_clusters=11, \n",
    "                        verbose=1, n_init=3, max_iter=300)\n",
    "clustering = km.fit(layer_outs_arr)\n",
    "predictions = clustering.predict(layer_outs_arr)\n",
    "predictions, predictions.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    for i in range(15):\n",
    "        for ima in np.array(os.listdir(dataset_dir))[np.where(predictions==i)]:\n",
    "            filepath = os.path.join(dataset_dir, ima)\n",
    "            img = cv2.imread(filepath)\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras import backend as K\n",
    "from keras.layers import merge, Input\n",
    "from keras.preprocessing import image\n",
    "cut_layer = 20\n",
    "\n",
    "\n",
    "image_input = Input(shape=(224, 224, 3))\n",
    "model = VGG16(include_top=True, weights=\"imagenet\",\n",
    "              input_tensor=image_input)  \n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Will cut at layer {}'.format(model.layers[cut_layer].output))\n",
    "\n",
    "inp = model.input  # input placeholder\n",
    "outputs = model.layers[cut_layer].output\n",
    "print(inp)\n",
    "print(outputs)\n",
    "functor = K.function([inp], [outputs])  # evaluation function\n",
    "\n",
    "layer_outs_arr = []\n",
    "\n",
    "for j, filename in enumerate(os.listdir(dataset_dir)):\n",
    "    \n",
    "    if filename == '.DStore':\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        filepath = os.path.join(dataset_dir, filename)\n",
    "        if (j % 100) == 0:\n",
    "            print(j, filepath)\n",
    "\n",
    "        img = image.load_img(filepath, target_size=(224, 224))\n",
    "        img_data = image.img_to_array(img)\n",
    "        img_data = np.expand_dims(img_data, axis=0)  # simply creates batch of size one as required by preprocess_input\n",
    "        img_data = tf.keras.applications.vgg16.preprocess_input(img_data)\n",
    "        layer_out = functor([img_data, 1])\n",
    "        layer_out = np.array(layer_out)\n",
    "        \n",
    "        layer_outs_arr.append(layer_out.flatten())\n",
    "\n",
    "layer_outs_arr = np.array(layer_outs_arr)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(init='k-means++', n_clusters=11, \n",
    "                        verbose=1, n_init=3, max_iter=300)\n",
    "clustering = km.fit(layer_outs_arr)\n",
    "predictions = clustering.predict(layer_outs_arr)\n",
    "predictions, predictions.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    for i in range(15):\n",
    "        for ima in np.array(os.listdir(dataset_dir))[np.where(predictions==i)]:\n",
    "            filepath = os.path.join(dataset_dir, ima)\n",
    "            img = cv2.imread(filepath)\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras import backend as K\n",
    "from keras.layers import merge, Input\n",
    "from keras.preprocessing import image\n",
    "cut_layer = 15\n",
    "\n",
    "\n",
    "image_input = Input(shape=(224, 224, 3))\n",
    "model = VGG16(include_top=True, weights=\"imagenet\",\n",
    "              input_tensor=image_input)  \n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Will cut at layer {}'.format(model.layers[cut_layer].output))\n",
    "\n",
    "inp = model.input  # input placeholder\n",
    "outputs = model.layers[cut_layer].output\n",
    "print(inp)\n",
    "print(outputs)\n",
    "functor = K.function([inp], [outputs])  # evaluation function\n",
    "\n",
    "layer_outs_arr = []\n",
    "\n",
    "for j, filename in enumerate(os.listdir(dataset_dir)):\n",
    "    \n",
    "    if filename == '.DStore':\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        filepath = os.path.join(dataset_dir, filename)\n",
    "        if (j % 100) == 0:\n",
    "            print(j, filepath)\n",
    "            \n",
    "        img = image.load_img(filepath, target_size=(112, 112)) # load image in every rotation pos\n",
    "        stimg = np.stack\n",
    "        img_data = image.img_to_array(img)\n",
    "        img_data1 = np.rot90(img_data)\n",
    "        img_data2 = np.rot90(img_data1)\n",
    "        img_data3 = np.rot90(img_data2)\n",
    "        \n",
    "        stimg = np.stack((img_data, img_data1, img_data2, img_data3))\n",
    "        stimg=stimg[None]\n",
    "        stimg =stimg.reshape(2, 2, 112, 112, 3)\n",
    "       \n",
    "        stimg =stimg.transpose(0,2,1,3,4)\n",
    "        stimg = stimg.reshape(224, 224, 3)\n",
    "        if (j == 0):\n",
    "            fig = plt.figure()\n",
    "            plt.imshow((stimg * 255).astype(np.uint8))\n",
    "            plt.show()\n",
    "            \n",
    "        img_data = np.expand_dims(stimg, axis=0)  # simply creates batch of size one as required by preprocess_input\n",
    "        img_data = tf.keras.applications.vgg16.preprocess_input(img_data)\n",
    "        layer_out = functor([img_data, 1])\n",
    "        layer_out = np.array(layer_out)\n",
    "        \n",
    "        layer_outs_arr.append(layer_out.flatten())\n",
    "\n",
    "layer_outs_arr = np.array(layer_outs_arr)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "km = KMeans(init='k-means++', n_clusters=10, \n",
    "                        verbose=1, n_init=3, max_iter=300)\n",
    "clustering = km.fit(layer_outs_arr)\n",
    "predictions = clustering.predict(layer_outs_arr)\n",
    "predictions, predictions.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axis=plt.subplots(15,5, figsize= (20,20))\n",
    " \n",
    "for i in range(15):\n",
    "    count = 0\n",
    "    for ima in np.array(os.listdir(dataset_dir))[np.where(predictions==i)]:\n",
    "        \n",
    "        ax=axis[i, count]\n",
    "        count+=1\n",
    "        filepath = os.path.join(dataset_dir, ima)\n",
    "        img = cv2.imread(filepath)\n",
    "        ax.imshow(img)\n",
    "        #plt.show()\n",
    "        if count == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
